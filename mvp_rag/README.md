# ğŸ¥ MVP RAG Healthcare AI Assistant

Minimal Retrievalâ€‘Augmented Generation demo using local tools.

## ğŸ¯ What This Demonstrates

- **MVP RAG System**: Not just a demo - actual semantic search with embeddings
- **Local AI Stack**: Ollama + Qdrant for cost-effective development
- **Production Patterns**: Proper architecture, error handling, and monitoring
- **Healthcare Focus**: Domain-specific AI assistant with medical disclaimers

## ğŸ—ï¸ Architecture

```mermaid
flowchart TD
    A[User Query] --> B[Gradio UI]
    B --> C[Query Embedding<br/>nomic-embed-text]
    C --> D[Qdrant Vector Store]
    D --> E[Semantic Search<br/>top_k=3, threshold=0.38]
    E --> F[Context Retrieval]
    F --> G[LLM Generation<br/>qwen3:4b-instruct]
    G --> H[Streaming Response + Citations]
    H --> B
    
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#bbf,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
    style D fill:#cfc,stroke:#333,stroke-width:2px
    style E fill:#ffc,stroke:#333,stroke-width:2px
    style F fill:#fcf,stroke:#333,stroke-width:2px
    style G fill:#ccf,stroke:#333,stroke-width:2px
    style H fill:#bbf,stroke:#333,stroke-width:2px
```

## ğŸš€ Quick Start

### Prerequisites
- **Docker Desktop** running
- **Ollama** running with required models:
  ```bash
  ollama pull qwen3:4b-instruct    # For text generation
  ollama pull nomic-embed-text      # For embeddings
  ```

### Run the Demo
```bash
# 1. Activate virtual environment
source ./venv/bin/activate

# 2. Start Qdrant vector database
docker rm -f qdrant 2>/dev/null || true
docker run -d --name qdrant -p 6333:6333 -p 6334:6334 qdrant/qdrant

# 3. Ingest sample healthcare documents
python data/ingest.py

# 4. Launch the beautiful Gradio UI
python app.py
```

ğŸŒ **Open your browser**: http://localhost:7860

## ğŸ“ Project Structure

```
mvp_rag/
â”œâ”€â”€ core/                    # Core RAG components
â”‚   â”œâ”€â”€ llm_client.py       # Ollama client for LLM & embeddings
â”‚   â”œâ”€â”€ vector_store.py     # Qdrant vector database client
â”‚   â””â”€â”€ rag_engine.py       # Main RAG orchestration engine
â”œâ”€â”€ data/                    # Data ingestion scripts
â”‚   â””â”€â”€ ingest.py           # Sample healthcare documents
â”œâ”€â”€ utils/                   # Utilities and logging
â”œâ”€â”€ app.py                   # Gradio web interface
â”œâ”€â”€ requirements.txt         # Python dependencies
â””â”€â”€ README.md               # This file
```

## ğŸ”§ Key Features

- **Smart Retrieval**: Top-3 most relevant documents with similarity threshold
- **Streaming UI**: Progressive response display for better UX
- **Performance Metrics**: Real-time timing and search analytics
- **Medical Safety**: Built-in disclaimers and professional consultation guidance
- **Auto-scaling**: Vector size mismatch detection and collection recreation

## ğŸ“Š Performance Optimizations

- **Embeddings**: `nomic-embed-text` for fast, accurate semantic search
- **Retrieval**: `top_k=3` with `score_threshold=0.38` for focused context
- **Generation**: Optimized prompts for concise, actionable responses
- **Pre-warming**: LLM initialization to reduce first-response latency

## ğŸ­ Demo Flow

1. **User enters health question** â†’ Gradio interface
2. **Query gets embedded** â†’ nomic-embed-text model
3. **Vector search** â†’ Qdrant finds similar healthcare documents
4. **Context preparation** â†’ Top 3 relevant documents selected
5. **LLM generation** â†’ qwen3:4b-instruct creates response
6. **Streaming display** â†’ Progressive response with citations
7. **Performance metrics** â†’ Timing and search analytics shown

## ğŸš€ Evolution Path

This MVP demonstrates the foundation for:
- **Enterprise RAG**: Azure AI Search + OpenAI
- **AI Agents**: Azure Agent Service + tool integration
- **Production Monitoring**: Azure AI Foundry + observability
- **Content Safety**: Azure Content Safety + guardrails

---

*Built with â¤ï¸ for demonstrating AI evolution from MVP to production*
