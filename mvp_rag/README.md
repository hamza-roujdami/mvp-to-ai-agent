# MVP RAG Healthcare AI Assistant

A production-ready RAG (Retrieval-Augmented Generation) system built with local AI tools, demonstrating the journey from MVP to enterprise Azure services.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User Query    â”‚â”€â”€â”€â–¶â”‚   Embeddings    â”‚â”€â”€â”€â–¶â”‚   Vector DB     â”‚
â”‚                 â”‚    â”‚   (bge-m3)      â”‚    â”‚   (Qdrant)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LLM Response  â”‚â—€â”€â”€â”€â”‚   Context       â”‚â—€â”€â”€â”€â”‚   Similarity    â”‚
â”‚   (phi4-mini)   â”‚    â”‚   Retrieval     â”‚    â”‚   Search        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Tech Stack

### **Local AI Tools:**
- **LLM**: Ollama (phi4-mini for responses)
- **Embeddings**: Ollama (bge-m3 for vector embeddings)
- **Vector DB**: Qdrant (local Docker container)
- **Framework**: LangChain (RAG orchestration)

### **Production Evolution Path:**
- **LLM**: Ollama â†’ Azure OpenAI (GPT-4)
- **Embeddings**: bge-m3 â†’ Azure OpenAI (text-embedding-ada-002)
- **Vector DB**: Qdrant â†’ Azure AI Search
- **Safety**: Basic â†’ Azure Content Safety

## ğŸ“ Project Structure

```
mvp_rag/
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ requirements.txt          # Dependencies
â”œâ”€â”€ config.py                 # Configuration
â”œâ”€â”€ main.py                   # Main demo script
â”œâ”€â”€ core/                     # Core RAG components
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ rag_engine.py         # Main RAG orchestration
â”‚   â”œâ”€â”€ llm_client.py         # Ollama LLM client
â”‚   â””â”€â”€ vector_store.py       # Qdrant vector operations
â”œâ”€â”€ data/                     # Healthcare data
â”‚   â”œâ”€â”€ documents/            # Source documents
â”‚   â”œâ”€â”€ embeddings/           # Pre-computed embeddings
â”‚   â””â”€â”€ knowledge_base.py     # Data management
â”œâ”€â”€ embeddings/               # Embedding utilities
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ embedder.py           # Text embedding logic
â”‚   â””â”€â”€ chunker.py            # Document chunking
â”œâ”€â”€ models/                   # Data models
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ document.py           # Document structure
â”‚   â””â”€â”€ response.py           # Response models
â””â”€â”€ utils/                    # Utilities
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ logger.py             # Logging
    â””â”€â”€ metrics.py            # Performance metrics
```

## ğŸ”„ RAG Flow

### **1. Document Ingestion**
```
Healthcare Documents â†’ Chunking â†’ Embedding â†’ Storage in Qdrant
```

### **2. Query Processing**
```
User Query â†’ Embed Query â†’ Vector Search â†’ Retrieve Top-K â†’ LLM Generation
```

### **3. Response Generation**
```
Context + Query â†’ LLM Prompt â†’ Generated Response â†’ Safety Check â†’ Final Answer
```

## ğŸ¯ Key Features

### **Current MVP:**
- âœ… Real RAG with local AI tools
- âœ… Semantic search with embeddings
- âœ… Dynamic LLM responses
- âœ… Healthcare knowledge base
- âœ… Performance metrics

### **Production Ready:**
- âœ… Clean architecture
- âœ… Modular design
- âœ… Easy Azure migration path
- âœ… Comprehensive logging
- âœ… Error handling

## ğŸš€ Quick Start

### **1. Prerequisites**
```bash
# Start Qdrant
docker run -d -p 6333:6333 -p 6334:6334 --name qdrant qdrant/qdrant

# Start Ollama
ollama serve

# Install models
ollama pull phi4-mini
ollama pull bge-m3
```

### **2. Setup**
```bash
# Install dependencies
pip install -r requirements.txt

# Run the MVP
python main.py
```

## ğŸ“Š Performance Metrics

- **Query Response Time**: End-to-end latency
- **Vector Search Quality**: Relevance scores
- **LLM Generation Time**: Response generation latency
- **Memory Usage**: Resource consumption
- **Accuracy**: Response quality assessment

## ğŸ”® Evolution Path

### **Week 1**: âœ… Local RAG MVP
- Ollama + Qdrant working
- Healthcare knowledge base
- Real RAG responses

### **Week 2**: ğŸš§ Azure Integration
- Azure OpenAI for LLM
- Azure AI Search for vectors
- Content Safety integration

### **Week 3**: ğŸš§ Production Features
- Agentic workflow
- Tool calling
- Observability

## ğŸ­ Demo Scenarios

1. **Basic Health Q&A**: "What are diabetes symptoms?"
2. **Complex Queries**: "How do I manage stress with exercise?"
3. **Context Retrieval**: Show retrieved documents
4. **Performance Metrics**: Response times, search quality
5. **Limitations**: What local tools can't do
6. **Azure Benefits**: What production brings

## ğŸ¤ Contributing

This MVP demonstrates local AI capabilities and provides a clear path to production Azure services.
